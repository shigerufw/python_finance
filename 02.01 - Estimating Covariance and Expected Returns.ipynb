{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimanting Covariance and Expected Returns\n",
    "\n",
    "We've backtested CW and EW Portfolios and they each have unique positions in industrial implementations.\n",
    "\n",
    "CapWeighted portfolios are the overall average and hence the de-facto industry standard. They are very inexpensive to implement and feature very low turnover. In many ways, they are the default go-to implementation choice for many investors.\n",
    "\n",
    "However, we've seen that they suffer from some disadvantages, so there is some room for improvement.\n",
    "\n",
    "EW are the most obvious improvement because they are the only other technique we are going to look at (other than CW) that requires no estimation of either covariance or expected returns. We've already backtested those and we've seen how easy they are to build.\n",
    "\n",
    "We'll now move on the more sophisticated portfolio construction techniques, but they will get us involved in the estimation game, something we've avoided so far ... so let's start by pulling in the data we need and start with the CW and EW portfolios, since they are the baseline portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_file(filetype, weighting=\"vw\", n_inds=30):\n",
    "    \"\"\"\n",
    "    Load and format the Ken French Industry Portfolios files\n",
    "    Variant is a tuple of (weighting, size) where:\n",
    "        weighting is one of \"ew\", \"vw\"\n",
    "        number of inds is 30 or 49\n",
    "    \"\"\"    \n",
    "    if filetype == \"returns\":\n",
    "        name = f\"{weighting}_rets\" \n",
    "        divisor = 100\n",
    "    elif filetype == \"nfirms\":\n",
    "        name = \"nfirms\"\n",
    "        divisor = 1\n",
    "    elif filetype == \"size\":\n",
    "        name = \"size\"\n",
    "        divisor = 1\n",
    "    else:\n",
    "        raise ValueError(f\"filetype must be one of: returns, nfirms, size\")\n",
    "    \n",
    "    ind = pd.read_csv(f\"data/ind{n_inds}_m_{name}.csv\", header=0, index_col=0, na_values=-99.99)/divisor\n",
    "    ind.index = pd.to_datetime(ind.index, format=\"%Y%m\").to_period('M')\n",
    "    ind.columns = ind.columns.str.strip()\n",
    "    return ind\n",
    "\n",
    "def get_ind_returns(weighting=\"vw\", n_inds=30):\n",
    "    \"\"\"\n",
    "    Load and format the Ken French Industry Portfolios Monthly Returns\n",
    "    \"\"\"\n",
    "    return get_ind_file(\"returns\", weighting=weighting, n_inds=n_inds)\n",
    "\n",
    "def get_ind_nfirms(n_inds=30):\n",
    "    \"\"\"\n",
    "    Load and format the Ken French 30 Industry Portfolios Average number of Firms\n",
    "    \"\"\"\n",
    "    return get_ind_file(\"nfirms\", n_inds=n_inds)\n",
    "\n",
    "def get_ind_size(n_inds=30):\n",
    "    \"\"\"\n",
    "    Load and format the Ken French 30 Industry Portfolios Average size (market cap)\n",
    "    \"\"\"\n",
    "    return get_ind_file(\"size\", n_inds=n_inds)\n",
    "\n",
    "\n",
    "def get_ind_market_caps(n_inds=30, weights=False):\n",
    "    \"\"\"\n",
    "    Load the industry portfolio data and derive the market caps\n",
    "    \"\"\"\n",
    "    ind_nfirms = get_ind_nfirms(n_inds=n_inds)\n",
    "    ind_size = get_ind_size(n_inds=n_inds)\n",
    "    ind_mktcap = ind_nfirms * ind_size\n",
    "    if weights:\n",
    "        total_mktcap = ind_mktcap.sum(axis=1)\n",
    "        ind_capweight = ind_mktcap.divide(total_mktcap, axis=\"rows\")\n",
    "        return ind_capweight\n",
    "    #else\n",
    "    return ind_mktcap\n",
    "\n",
    "def get_fff_returns():\n",
    "    \"\"\"\n",
    "    Load the Fama-French Research Factor Monthly Dataset\n",
    "    Mkt-Rf = market - risk free\n",
    "    SMB = Small - Big\n",
    "    HML = Hight - Low\n",
    "    \"\"\"\n",
    "    rets = pd.read_csv(\"./data/F-F_Research_Data_Factors_m.csv\",\n",
    "                       header=0, index_col=0, na_values=-99.99)/100\n",
    "    rets.index = pd.to_datetime(rets.index, format=\"%Y%m\").to_period('M')\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = ['Food', 'Beer', 'Smoke', 'Games', 'Books', 'Hshld', 'Clths', 'Hlth',\n",
    "       'Chems', 'Txtls', 'Cnstr', 'Steel', 'FabPr', 'ElcEq', 'Autos', 'Carry',\n",
    "       'Mines', 'Coal', 'Oil', 'Util', 'Telcm', 'Servs', 'BusEq', 'Paper',\n",
    "       'Trans', 'Whlsl', 'Rtail', 'Meals', 'Fin', 'Other']\n",
    "#inds = ['Beer','Hlth', 'Rtail', 'Whlsl']\n",
    "ind_rets = get_ind_returns(weighting='ew', n_inds = 49)['1974':]\n",
    "ind_mcap = get_ind_market_caps(49, weights=True)['1974':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_ew(r, cap_weights=None, max_cw_mult=None, microcap_threshold=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the weights of the EW portfolio based on the asset returns \"r\" as a DataFrame\n",
    "    If supplied a set of capweights and a capweight tether, it is applied and reweighted \n",
    "    \"\"\"\n",
    "    n = len(r.columns)\n",
    "    ew = pd.Series(1/n, index=r.columns)\n",
    "    if cap_weights is not None:\n",
    "        cw = cap_weights.loc[r.index[0]] # starting cap weight\n",
    "        ## exclude microcaps\n",
    "        if microcap_threshold is not None and microcap_threshold > 0:\n",
    "            microcap = cw < microcap_threshold\n",
    "            ew[microcap] = 0\n",
    "            ew = ew/ew.sum()\n",
    "        #limit weight to a multiple of capweight\n",
    "        if max_cw_mult is not None and max_cw_mult > 0:\n",
    "            ew = np.minimum(ew, cw*max_cw_mult)\n",
    "            ew = ew/ew.sum() #reweight\n",
    "    return ew\n",
    "\n",
    "def weight_cw(r, cap_weights, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the weights of the CW portfolio based on the time series of capweights\n",
    "    \"\"\"\n",
    "    w = cap_weights.loc[r.index[0]]\n",
    "    return w/w.sum()\n",
    "\n",
    "\n",
    "def backtest_ws(r, estimation_window=60, weighting=weight_ew, verbose=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Backtests a given weighting scheme, given some parameters:\n",
    "    r : asset returns to use to build the portfolio\n",
    "    estimation_window: the window to use to estimate parameters\n",
    "    weighting: the weighting scheme to use, must be a function that takes \"r\", and a variable number of keyword-value arguments\n",
    "    \"\"\"\n",
    "    n_periods = r.shape[0]\n",
    "    # return windows\n",
    "    windows = [(start, start+estimation_window) for start in range(n_periods-estimation_window)]\n",
    "    weights = [weighting(r.iloc[win[0]:win[1]], **kwargs) for win in windows]\n",
    "    # convert List of weights to DataFrame\n",
    "    weights = pd.DataFrame(weights, index=r.iloc[estimation_window:].index, columns=r.columns)\n",
    "    returns = (weights * r).sum(axis=\"columns\",  min_count=1) #mincount is to generate NAs if all inputs are NAs\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "def skewness(r):\n",
    "    \"\"\"\n",
    "    Alternative to scipy.stats.skew()\n",
    "    Computes the skewness of the supplied Series or DataFrame\n",
    "    Returns a float or a Series\n",
    "    \"\"\"\n",
    "    demeaned_r = r - r.mean()\n",
    "    # use the population standard deviation, so set dof=0\n",
    "    sigma_r = r.std(ddof=0)\n",
    "    exp = (demeaned_r**3).mean()\n",
    "    return exp/sigma_r**3\n",
    "\n",
    "\n",
    "def kurtosis(r):\n",
    "    \"\"\"\n",
    "    Alternative to scipy.stats.kurtosis()\n",
    "    Computes the kurtosis of the supplied Series or DataFrame\n",
    "    Returns a float or a Series\n",
    "    \"\"\"\n",
    "    demeaned_r = r - r.mean()\n",
    "    # use the population standard deviation, so set dof=0\n",
    "    sigma_r = r.std(ddof=0)\n",
    "    exp = (demeaned_r**4).mean()\n",
    "    return exp/sigma_r**4\n",
    "\n",
    "def drawdown(return_series: pd.Series):\n",
    "    \"\"\"Takes a time series of asset returns.\n",
    "       returns a DataFrame with columns for\n",
    "       the wealth index, \n",
    "       the previous peaks, and \n",
    "       the percentage drawdown\n",
    "    \"\"\"\n",
    "    wealth_index = 1000*(1+return_series).cumprod()\n",
    "    previous_peaks = wealth_index.cummax()\n",
    "    drawdowns = (wealth_index - previous_peaks)/previous_peaks\n",
    "    return pd.DataFrame({\"Wealth\": wealth_index, \n",
    "                         \"Previous Peak\": previous_peaks, \n",
    "                         \"Drawdown\": drawdowns})\n",
    "\n",
    "\n",
    "\n",
    "def compound(r):\n",
    "    \"\"\"\n",
    "    returns the result of compounding the set of returns in r\n",
    "    \"\"\"\n",
    "    return np.expm1(np.log1p(r).sum())\n",
    "\n",
    "                         \n",
    "def annualize_rets(r, periods_per_year):\n",
    "    \"\"\"\n",
    "    Annualizes a set of returns\n",
    "    We should infer the periods per year\n",
    "    but that is currently left as an exercise\n",
    "    to the reader :-)\n",
    "    \"\"\"\n",
    "    compounded_growth = (1+r).prod()\n",
    "    n_periods = r.shape[0]\n",
    "    return compounded_growth**(periods_per_year/n_periods)-1\n",
    "\n",
    "\n",
    "def annualize_vol(r, periods_per_year):\n",
    "    \"\"\"\n",
    "    Annualizes the vol of a set of returns\n",
    "    We should infer the periods per year\n",
    "    but that is currently left as an exercise\n",
    "    to the reader :-)\n",
    "    \"\"\"\n",
    "    return r.std()*(periods_per_year**0.5)\n",
    "\n",
    "\n",
    "def sharpe_ratio(r, riskfree_rate, periods_per_year):\n",
    "    \"\"\"\n",
    "    Computes the annualized sharpe ratio of a set of returns\n",
    "    \"\"\"\n",
    "    # convert the annual riskfree rate to per period\n",
    "    rf_per_period = (1+riskfree_rate)**(1/periods_per_year)-1\n",
    "    excess_ret = r - rf_per_period\n",
    "    ann_ex_ret = annualize_rets(excess_ret, periods_per_year)\n",
    "    ann_vol = annualize_vol(r, periods_per_year)\n",
    "    return ann_ex_ret/ann_vol\n",
    "\n",
    "\n",
    "\n",
    "def is_normal(r, level=0.01):\n",
    "    \"\"\"\n",
    "    Applies the Jarque-Bera test to determine if a Series is normal or not\n",
    "    Test is applied at the 1% level by default\n",
    "    Returns True if the hypothesis of normality is accepted, False otherwise\n",
    "    \"\"\"\n",
    "    if isinstance(r, pd.DataFrame):\n",
    "        return r.aggregate(is_normal)\n",
    "    else:\n",
    "        statistic, p_value = scipy.stats.jarque_bera(r)\n",
    "        return p_value > level\n",
    "\n",
    "\n",
    "def semideviation(r):\n",
    "    \"\"\"\n",
    "    Returns the semideviation aka negative semideviation of r\n",
    "    r must be a Series or a DataFrame, else raises a TypeError\n",
    "    \"\"\"\n",
    "    if isinstance(r, pd.Series):\n",
    "        is_negative = r < 0\n",
    "        return r[is_negative].std(ddof=0)\n",
    "    elif isinstance(r, pd.DataFrame):\n",
    "        return r.aggregate(semideviation)\n",
    "    else:\n",
    "        raise TypeError(\"Expected r to be a Series or DataFrame\")\n",
    "\n",
    "\n",
    "def var_historic(r, level=5):\n",
    "    \"\"\"\n",
    "    Returns the historic Value at Risk at a specified level\n",
    "    i.e. returns the number such that \"level\" percent of the returns\n",
    "    fall below that number, and the (100-level) percent are above\n",
    "    \"\"\"\n",
    "    if isinstance(r, pd.DataFrame):\n",
    "        return r.aggregate(var_historic, level=level)\n",
    "    elif isinstance(r, pd.Series):\n",
    "        return -np.percentile(r, level)\n",
    "    else:\n",
    "        raise TypeError(\"Expected r to be a Series or DataFrame\")\n",
    "\n",
    "\n",
    "def cvar_historic(r, level=5):\n",
    "    \"\"\"\n",
    "    Computes the Conditional VaR of Series or DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(r, pd.Series):\n",
    "        is_beyond = r <= -var_historic(r, level=level)\n",
    "        return -r[is_beyond].mean()\n",
    "    elif isinstance(r, pd.DataFrame):\n",
    "        return r.aggregate(cvar_historic, level=level)\n",
    "    else:\n",
    "        raise TypeError(\"Expected r to be a Series or DataFrame\")\n",
    "\n",
    "\n",
    "\n",
    "def var_gaussian(r, level=5, modified=False):\n",
    "    \"\"\"\n",
    "    Returns the Parametric Gauusian VaR of a Series or DataFrame\n",
    "    If \"modified\" is True, then the modified VaR is returned,\n",
    "    using the Cornish-Fisher modification\n",
    "    \"\"\"\n",
    "    # compute the Z score assuming it was Gaussian\n",
    "    z = norm.ppf(level/100)\n",
    "    if modified:\n",
    "        # modify the Z score based on observed skewness and kurtosis\n",
    "        s = skewness(r)\n",
    "        k = kurtosis(r)\n",
    "        z = (z +\n",
    "                (z**2 - 1)*s/6 +\n",
    "                (z**3 -3*z)*(k-3)/24 -\n",
    "                (2*z**3 - 5*z)*(s**2)/36\n",
    "            )\n",
    "    return -(r.mean() + z*r.std(ddof=0))\n",
    "    \n",
    "def summary_stats(r, riskfree_rate=0.03):\n",
    "    \"\"\"\n",
    "    Return a DataFrame that contains aggregated summary stats for the returns in the columns of r\n",
    "    \"\"\"\n",
    "    ann_r = r.aggregate(annualize_rets, periods_per_year=12)\n",
    "    ann_vol = r.aggregate(annualize_vol, periods_per_year=12)\n",
    "    ann_sr = r.aggregate(sharpe_ratio, riskfree_rate=riskfree_rate, periods_per_year=12)\n",
    "    dd = r.aggregate(lambda r: drawdown(r).Drawdown.min())\n",
    "    skew = r.aggregate(skewness)\n",
    "    kurt = r.aggregate(kurtosis)\n",
    "    cf_var5 = r.aggregate(var_gaussian, modified=True)\n",
    "    hist_cvar5 = r.aggregate(cvar_historic)\n",
    "    return pd.DataFrame({\n",
    "        \"Annualized Return\": ann_r,\n",
    "        \"Annualized Vol\": ann_vol,\n",
    "        \"Skewness\": skew,\n",
    "        \"Kurtosis\": kurt,\n",
    "        \"Cornish-Fisher VaR (5%)\": cf_var5,\n",
    "        \"Historic CVaR (5%)\": hist_cvar5,\n",
    "        \"Sharpe Ratio\": ann_sr,\n",
    "        \"Max Drawdown\": dd\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewr = backtest_ws(ind_rets, estimation_window=36, weighting=weight_ew)\n",
    "cwr = backtest_ws(ind_rets, estimation_window=36, weighting=weight_cw, cap_weights=ind_mcap)\n",
    "btr = pd.DataFrame({\"EW\": ewr, \"CW\": cwr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annualized Return</th>\n",
       "      <th>Annualized Vol</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Cornish-Fisher VaR (5%)</th>\n",
       "      <th>Historic CVaR (5%)</th>\n",
       "      <th>Sharpe Ratio</th>\n",
       "      <th>Max Drawdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EW</th>\n",
       "      <td>0.131606</td>\n",
       "      <td>0.187437</td>\n",
       "      <td>-0.616296</td>\n",
       "      <td>6.771301</td>\n",
       "      <td>0.082035</td>\n",
       "      <td>0.122226</td>\n",
       "      <td>0.527384</td>\n",
       "      <td>-0.598060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CW</th>\n",
       "      <td>0.131382</td>\n",
       "      <td>0.187040</td>\n",
       "      <td>-0.555883</td>\n",
       "      <td>5.825438</td>\n",
       "      <td>0.082040</td>\n",
       "      <td>0.120070</td>\n",
       "      <td>0.527341</td>\n",
       "      <td>-0.589473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Annualized Return  Annualized Vol  Skewness  Kurtosis  \\\n",
       "EW           0.131606        0.187437 -0.616296  6.771301   \n",
       "CW           0.131382        0.187040 -0.555883  5.825438   \n",
       "\n",
       "    Cornish-Fisher VaR (5%)  Historic CVaR (5%)  Sharpe Ratio  Max Drawdown  \n",
       "EW                 0.082035            0.122226      0.527384     -0.598060  \n",
       "CW                 0.082040            0.120070      0.527341     -0.589473  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_stats(btr.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1+btr).cumprod().plot(figsize=(12,6), title=\"Industry Portfolios - CW vs EW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Global Minimum Variance Portifolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msr(riskfree_rate, er, cov):\n",
    "    \"\"\"\n",
    "    Returns the weights of the portfolio that gives you the maximum sharpe ratio\n",
    "    given the riskfree rate and expected returns and a covariance matrix\n",
    "    \"\"\"\n",
    "    n = er.shape[0]\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    bounds = ((0.0, 1.0),) * n # an N-tuple of 2-tuples!\n",
    "    # construct the constraints\n",
    "    weights_sum_to_1 = {'type': 'eq',\n",
    "                        'fun': lambda weights: np.sum(weights) - 1\n",
    "    }\n",
    "    def neg_sharpe(weights, riskfree_rate, er, cov):\n",
    "        \"\"\"\n",
    "        Returns the negative of the sharpe ratio\n",
    "        of the given portfolio\n",
    "        \"\"\"\n",
    "        r = portfolio_return(weights, er)\n",
    "        vol = portfolio_vol(weights, cov)\n",
    "        return -(r - riskfree_rate)/vol\n",
    "    \n",
    "    weights = minimize(neg_sharpe, init_guess,\n",
    "                       args=(riskfree_rate, er, cov), method='SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints=(weights_sum_to_1,),\n",
    "                       bounds=bounds)\n",
    "    return weights.x\n",
    "\n",
    "def gmv(cov):\n",
    "    \"\"\"\n",
    "    Returns the weights of the Global Minimum Volatility portfolio\n",
    "    given a covariance matrix\n",
    "    \"\"\"\n",
    "    n = cov.shape[0]\n",
    "    return msr(0, np.repeat(1, n), cov)\n",
    "\n",
    "def sample_cov(r, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the sample covariance of the supplied returns\n",
    "    \"\"\"\n",
    "    return r.cov()\n",
    "\n",
    "def weight_gmv(r, cov_estimator=sample_cov, **kwargs):\n",
    "    \"\"\"\n",
    "    Produces the weights of the GMV portfolio given a covariance matrix of the returns \n",
    "    \"\"\"\n",
    "    est_cov = cov_estimator(r, **kwargs)\n",
    "    return gmv(est_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_return(weights, returns):\n",
    "    \"\"\"\n",
    "    Computes the return on a portfolio from constituent returns and weights\n",
    "    weights are a numpy array or Nx1 matrix and returns are a numpy array or Nx1 matrix\n",
    "    \"\"\"\n",
    "    return weights.T @ returns\n",
    "\n",
    "def portfolio_vol(weights, covmat):\n",
    "    \"\"\"\n",
    "    Computes the vol of a portfolio from a covariance matrix and constituent weights\n",
    "    weights are a numpy array or N x 1 maxtrix and covmat is an N x N matrix\n",
    "    \"\"\"\n",
    "    vol = (weights.T @ covmat @ weights)**0.5\n",
    "    return vol \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_s_r = backtest_ws(ind_rets, estimation_window=36, weighting=weight_gmv, cov_estimator=sample_cov)\n",
    "btr = pd.DataFrame({\"EW\": ewr, \"CW\": cwr, \"GMV-Sample\": mv_s_r})\n",
    "(1+btr).cumprod().plot(figsize=(12,6), title=\"Industry Portfolios\")\n",
    "summary_stats(btr.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a new estimator - Constant Correlation. The idea is simple, take the sample correlation matrix, compute the average correlation and then reconstruct the covariance matrix. The relation between correlations $\\rho$ and covariance $\\sigma$ is given by:\n",
    "\n",
    "$$ \\rho_{ij} = \\frac{ \\sigma_{ij} } { \\sqrt{ \\sigma_{ii}  \\sigma_{jj} } } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statsmodels.stats.moment_helpers as mh\n",
    "def cc_cov(r, **kwargs):\n",
    "    \"\"\"\n",
    "    Estimates a covariance matrix by using the Elton/Gruber Constant Correlation model\n",
    "    \"\"\"\n",
    "    rhos = r.corr()\n",
    "    n = rhos.shape[0]\n",
    "    # this is a symmetric matrix with diagonals all 1 - so the mean correlation is ...\n",
    "    rho_bar = (rhos.values.sum()-n)/(n*(n-1))\n",
    "    ccor = np.full_like(rhos, rho_bar)\n",
    "    np.fill_diagonal(ccor, 1.)\n",
    "    sd = r.std()\n",
    "    ccov = ccor * np.outer(sd, sd)\n",
    "#     mh.corr2cov(ccor, sd)\n",
    "    return pd.DataFrame(ccov, index=r.columns, columns=r.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = pd.DataFrame({\n",
    "    \"EW\": weight_ew(ind_rets[\"2016\":]),\n",
    "    \"CW\": weight_cw(ind_rets[\"2016\":], cap_weights=ind_mcap),\n",
    "    \"GMV-Sample\": weight_gmv(ind_rets[\"2016\":], cov_estimator=sample_cov),\n",
    "    \"GMV-ConstCorr\": weight_gmv(ind_rets[\"2016\":], cov_estimator=cc_cov),\n",
    "})\n",
    "wts.T.plot.bar(stacked=True, figsize=(15,6), legend=False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_cc_r = backtest_ws(ind_rets, estimation_window=36, weighting=weight_gmv, cov_estimator=cc_cov)\n",
    "btr = pd.DataFrame({\"EW\": ewr, \"CW\": cwr, \"GMV-Sample\": mv_s_r, \"GMV-CC\": mv_cc_r})\n",
    "(1+btr).cumprod().plot(figsize=(12,6), title=\"Industry Portfolios\")\n",
    "summary_stats(btr.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrinkage_cov(r, delta=0.5, **kwargs):\n",
    "    \"\"\"\n",
    "    Covariance estimator that shrinks between the Sample Covariance and the Constant Correlation Estimators\n",
    "    \"\"\"\n",
    "    prior = cc_cov(r, **kwargs)\n",
    "    sample = sample_cov(r, **kwargs)\n",
    "    return delta*prior + (1-delta)*sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = pd.DataFrame({\n",
    "    \"EW\": weight_ew(ind_rets[\"2013\":]),\n",
    "    \"CW\": weight_cw(ind_rets[\"2013\":], cap_weights=ind_mcap),\n",
    "    \"GMV-Sample\": weight_gmv(ind_rets[\"2013\":], cov_estimator=sample_cov),\n",
    "    \"GMV-ConstCorr\": weight_gmv(ind_rets[\"2013\":], cov_estimator=cc_cov),\n",
    "    \"GMV-Shrink 0.5\": weight_gmv(ind_rets[\"2013\":], cov_estimator=shrinkage_cov),\n",
    "})\n",
    "wts.T.plot.bar(stacked=True, figsize=(12,6), legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_sh_r = backtest_ws(ind_rets, estimation_window=36, weighting=weight_gmv, cov_estimator=shrinkage_cov, delta=0.5)\n",
    "btr = pd.DataFrame({\"EW\": ewr, \"CW\": cwr, \"GMV-Sample\": mv_s_r, \"GMV-CC\": mv_cc_r, 'GMV-Shrink 0.5': mv_sh_r})\n",
    "(1+btr).cumprod().plot(figsize=(12,6), title=\"49 Industry Portfolios\")\n",
    "summary_stats(btr.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
